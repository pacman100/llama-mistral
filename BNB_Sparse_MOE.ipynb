{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6283a6b-717f-4493-99eb-110b8cca5212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== created Mixtral 8x7B. Experts spread over 2 GPUs ===\n",
      "Loaded in 488.12 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "from llama import Llama\n",
    "\n",
    "generator = Llama.build(\n",
    "        ckpt_dir=\"/raid/sourab/mixtral-8x7b-32kseqlen/\",\n",
    "        tokenizer_path=\"/raid/sourab/mixtral-8x7b-32kseqlen/tokenizer.model\",\n",
    "        max_seq_len=128,\n",
    "        max_batch_size=4,\n",
    "        num_gpus=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a638a2-4d1b-469f-acca-3a1fc91b26a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46702792704"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = 0\n",
    "for p in generator.model.parameters():\n",
    "    params += p.numel()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a481b1-aa83-452b-aaf2-a941aaef59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a1b65a-798e-4af2-b3b8-4868561be418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-7.4938e-38,  1.2214e-38, -5.7305e-37,  ..., -1.8220e-37,\n",
       "         -2.9534e-37, -2.1894e-37],\n",
       "        [-1.4954e-02,  2.5940e-04, -1.6113e-02,  ...,  2.2531e-05,\n",
       "         -1.6846e-02,  2.1973e-03],\n",
       "        [-5.3883e-05,  3.1948e-05,  6.3777e-06,  ...,  4.0293e-05,\n",
       "          1.1623e-05,  1.1474e-06],\n",
       "        ...,\n",
       "        [ 8.7891e-03, -1.3000e-02,  5.2795e-03,  ...,  5.5237e-03,\n",
       "          3.7079e-03,  3.7231e-03],\n",
       "        [ 1.9073e-03,  1.8677e-02, -4.9133e-03,  ...,  1.3245e-02,\n",
       "          4.6997e-03,  5.6152e-03],\n",
       "        [ 3.6926e-03,  1.6235e-02,  1.8597e-04,  ..., -7.7820e-03,\n",
       "         -9.5215e-03, -2.4292e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tok_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4280909-4fa5-41d0-962d-0b01da5e8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from llama.model import MoE\n",
    "import torch\n",
    "\n",
    "\n",
    "def replace_remaining_with_4bit(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear) and not isinstance(module, bnb.nn.modules.LinearSparse) and \"output\" not in name and \"gate\" not in name:\n",
    "            weight = module.weight.data\n",
    "            in_features = module.in_features\n",
    "            out_features = module.out_features\n",
    "            if module.bias is not None:\n",
    "                    bias = module.bias\n",
    "            module = bnb.nn.Linear4bit(\n",
    "                            in_features,\n",
    "                            out_features,\n",
    "                            module.bias is not None,\n",
    "                        )\n",
    "            module.weight = bnb.nn.Params4bit(weight, requires_grad=False)\n",
    "            if module.bias is not None:\n",
    "                    module.bias = bias\n",
    "            model._modules[name] = module\n",
    "            \n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_remaining_with_4bit(module)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def replace_moe_with_sparse_linear_and_remaining_with_4bit(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, MoE):\n",
    "            for expert in module.experts:\n",
    "                for name, layer in expert.named_children():\n",
    "                    weight = layer.weight.data\n",
    "                    if layer.bias is not None:\n",
    "                        bias = layer.bias\n",
    "                    in_features = layer.in_features\n",
    "                    out_features = layer.out_features\n",
    "                    layer = bnb.nn.modules.LinearSparse(in_features, out_features, layer.bias is not None)\n",
    "                    layer.weight = bnb.nn.modules.ParamsSparse(weight)\n",
    "                    if layer.bias is not None:\n",
    "                        layer.bias = bias\n",
    "                    setattr(expert, name, layer)\n",
    "    replace_remaining_with_4bit(model)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90de740-d765-4c80-be41-648402ba6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 22s, sys: 1min 9s, total: 6min 31s\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replace_moe_with_sparse_linear_and_remaining_with_4bit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9822049f-ae13-4412-befc-1ce69b52fbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(32000, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        (wk): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "        (wv): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "        (wo): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (feed_forward): MoE(\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x FeedForward(\n",
       "            (w1): LinearSparse(in_features=4096, out_features=14336, bias=False)\n",
       "            (w2): LinearSparse(in_features=14336, out_features=4096, bias=False)\n",
       "            (w3): LinearSparse(in_features=4096, out_features=14336, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547787f2-3ed0-471c-aefc-c604a7f24190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(32000, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        (wk): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "        (wv): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "        (wo): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (feed_forward): MoE(\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x FeedForward(\n",
       "            (w1): LinearSparse(in_features=4096, out_features=14336, bias=False)\n",
       "            (w2): LinearSparse(in_features=14336, out_features=4096, bias=False)\n",
       "            (w3): LinearSparse(in_features=4096, out_features=14336, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86329d96-92bd-40fc-8f8f-599c91e7f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral.ai is a company that\n",
      ">                  ,       ,       , ,      ,   ,     a a,     ,s ,\n",
      "\n",
      "==================================\n",
      "\n",
      "Simply put, the theory of relativity states that \n",
      ">                          ,   ,   , s,  ,  ,s,  ,   ,s,s,s ,s,s,\n",
      "\n",
      "==================================\n",
      "\n",
      "A brief message congratulating the team on the launch:\n",
      "\n",
      "Hi everyone,\n",
      "\n",
      "I just \n",
      ">   ,         ,  ,   ,   s , -,s,s s,s,s,s,s.s,s,s.s,s,s,s.s,s,s,\n",
      "\n",
      "==================================\n",
      "\n",
      "Translate English to French:\n",
      "\n",
      "sea otter => loutre de mer\n",
      "peppermint => menthe poivrée\n",
      "plush girafe => girafe peluche\n",
      "cheese =>\n",
      "> s,s,s,s.s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,s,\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "# For these prompts, the expected answer is the natural continuation of the prompt\n",
    "\"Mistral.ai is a company that\",\n",
    "\"Simply put, the theory of relativity states that \",\n",
    "\"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "Hi everyone,\n",
    "\n",
    "I just \"\"\",\n",
    "# Few shot prompt (providing a few examples before asking model to complete more);\n",
    "\"\"\"Translate English to French:\n",
    "\n",
    "sea otter => loutre de mer\n",
    "peppermint => menthe poivrée\n",
    "plush girafe => girafe peluche\n",
    "cheese =>\"\"\",\n",
    "]\n",
    "with torch.autocast(dtype=torch.float16, device_type=\"cuda\"):\n",
    "    results = generator.text_completion(\n",
    "    prompts,\n",
    "    max_gen_len=64,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    )\n",
    "    for prompt, result in zip(prompts, results):\n",
    "        print(prompt)\n",
    "        print(f\"> {result['generation']}\")\n",
    "        print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857952ed-736f-4898-b6c6-6dc50c47074a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
